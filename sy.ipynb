{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "import logging\n",
    "import pickle\n",
    "import requests, json\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restructure Prolog query\n",
    "\n",
    "e.g. writes(Who, hamlet) -> Who writes hamlet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_prolog(query):\n",
    "    # parse\n",
    "    predicate = query.split('(')[0]\n",
    "    arguments = query.split('(')[1][:-1]\n",
    "    arguments = [arg.strip() for arg in arguments.split(',')]\n",
    "    \n",
    "    # remove underline\n",
    "    # arguments = [arg.replace('_', ' ') for arg in arguments]\n",
    "    # predicate = predicate.replace('_', ' ')\n",
    "    \n",
    "    rtn = ['[CLS]', arguments[0], predicate] + arguments[1:] + ['.', '[SEP]']\n",
    "    \n",
    "    # mask\n",
    "    for i in range(len(rtn)):\n",
    "        if rtn[i][0].isupper():\n",
    "            rtn[i] = '[MASK]'\n",
    "    \n",
    "    return ' '.join(rtn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] [MASK] is the fifth planet from the Sun . [SEP]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restructure_prolog('is(X, the fifth planet from the Sun)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.data.datasets.language_modeling:Creating features from dataset file at ../dataset/geo/trainset.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "print('loading data')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data/trainset.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "#train_data, test_data = train_test_split(dataset, test_size=0.2, random_state=0)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.3\n",
    ")\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "fine_tuning_model = BertForMaskedLM.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.training_args:PyTorch: setting up devices\n",
      "INFO:transformers.trainer:You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "WARNING:transformers.training_args:Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "INFO:transformers.trainer:***** Running training *****\n",
      "INFO:transformers.trainer:  Num examples = 43624\n",
      "INFO:transformers.trainer:  Num Epochs = 3\n",
      "INFO:transformers.trainer:  Instantaneous batch size per device = 8\n",
      "INFO:transformers.trainer:  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "INFO:transformers.trainer:  Gradient Accumulation steps = 1\n",
      "INFO:transformers.trainer:  Total optimization steps = 65436\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b59851bdde24b1faaf2a1ed18b87bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=3, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296d78df4e564bd1a1e6b27d9d0a60fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=21812, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.6600315257534386, \"learning_rate\": 1.984717892291705e-05, \"epoch\": 0.02292316156244269, \"step\": 500}\n",
      "{\"loss\": 2.656905919849873, \"learning_rate\": 1.96943578458341e-05, \"epoch\": 0.04584632312488538, \"step\": 1000}\n",
      "{\"loss\": 2.5357310264110566, \"learning_rate\": 1.954153676875115e-05, \"epoch\": 0.06876948468732808, \"step\": 1500}\n",
      "{\"loss\": 2.5893159299492834, \"learning_rate\": 1.9388715691668196e-05, \"epoch\": 0.09169264624977076, \"step\": 2000}\n",
      "{\"loss\": 2.6749846221208573, \"learning_rate\": 1.9235894614585243e-05, \"epoch\": 0.11461580781221346, \"step\": 2500}\n",
      "{\"loss\": 2.559907356619835, \"learning_rate\": 1.9083073537502293e-05, \"epoch\": 0.13753896937465615, \"step\": 3000}\n",
      "{\"loss\": 2.5717898807525637, \"learning_rate\": 1.8930252460419344e-05, \"epoch\": 0.16046213093709885, \"step\": 3500}\n",
      "{\"loss\": 2.523244404554367, \"learning_rate\": 1.877743138333639e-05, \"epoch\": 0.18338529249954152, \"step\": 4000}\n",
      "{\"loss\": 2.5903904271603095, \"learning_rate\": 1.8624610306253438e-05, \"epoch\": 0.20630845406198423, \"step\": 4500}\n",
      "{\"loss\": 2.5174205047488214, \"learning_rate\": 1.8471789229170488e-05, \"epoch\": 0.22923161562442693, \"step\": 5000}\n",
      "{\"loss\": 2.5138180746138095, \"learning_rate\": 1.8318968152087538e-05, \"epoch\": 0.2521547771868696, \"step\": 5500}\n",
      "{\"loss\": 2.4522834317088127, \"learning_rate\": 1.8166147075004585e-05, \"epoch\": 0.2750779387493123, \"step\": 6000}\n",
      "{\"loss\": 2.567355050086975, \"learning_rate\": 1.8013325997921636e-05, \"epoch\": 0.298001100311755, \"step\": 6500}\n",
      "{\"loss\": 2.4962415383253247, \"learning_rate\": 1.7860504920838682e-05, \"epoch\": 0.3209242618741977, \"step\": 7000}\n",
      "{\"loss\": 2.4500847097933294, \"learning_rate\": 1.7707683843755733e-05, \"epoch\": 0.3438474234366404, \"step\": 7500}\n",
      "{\"loss\": 2.513587869286537, \"learning_rate\": 1.755486276667278e-05, \"epoch\": 0.36677058499908305, \"step\": 8000}\n",
      "{\"loss\": 2.5711849406957628, \"learning_rate\": 1.740204168958983e-05, \"epoch\": 0.3896937465615258, \"step\": 8500}\n",
      "{\"loss\": 2.412373568892479, \"learning_rate\": 1.7249220612506877e-05, \"epoch\": 0.41261690812396845, \"step\": 9000}\n",
      "{\"loss\": 2.4941150251626967, \"learning_rate\": 1.7096399535423927e-05, \"epoch\": 0.4355400696864111, \"step\": 9500}\n",
      "{\"loss\": 2.4911976992487905, \"learning_rate\": 1.6943578458340978e-05, \"epoch\": 0.45846323124885385, \"step\": 10000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-10000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-10000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-10000\\pytorch_model.bin\n",
      "C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python37\\site-packages\\torch\\optim\\lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.5578433197140695, \"learning_rate\": 1.6790757381258025e-05, \"epoch\": 0.4813863928112965, \"step\": 10500}\n",
      "{\"loss\": 2.4669101606309414, \"learning_rate\": 1.663793630417507e-05, \"epoch\": 0.5043095543737393, \"step\": 11000}\n",
      "{\"loss\": 2.451165301144123, \"learning_rate\": 1.6485115227092122e-05, \"epoch\": 0.5272327159361819, \"step\": 11500}\n",
      "{\"loss\": 2.364134666264057, \"learning_rate\": 1.6332294150009172e-05, \"epoch\": 0.5501558774986246, \"step\": 12000}\n",
      "{\"loss\": 2.4243748498335482, \"learning_rate\": 1.617947307292622e-05, \"epoch\": 0.5730790390610673, \"step\": 12500}\n",
      "{\"loss\": 2.4457915949225426, \"learning_rate\": 1.6026651995843266e-05, \"epoch\": 0.59600220062351, \"step\": 13000}\n",
      "{\"loss\": 2.4201494171917437, \"learning_rate\": 1.5873830918760316e-05, \"epoch\": 0.6189253621859527, \"step\": 13500}\n",
      "{\"loss\": 2.4789063927344976, \"learning_rate\": 1.5721009841677367e-05, \"epoch\": 0.6418485237483954, \"step\": 14000}\n",
      "{\"loss\": 2.3562137131989003, \"learning_rate\": 1.5568188764594414e-05, \"epoch\": 0.6647716853108381, \"step\": 14500}\n",
      "{\"loss\": 2.4392440321445465, \"learning_rate\": 1.541536768751146e-05, \"epoch\": 0.6876948468732808, \"step\": 15000}\n",
      "{\"loss\": 2.475980865985155, \"learning_rate\": 1.526254661042851e-05, \"epoch\": 0.7106180084357234, \"step\": 15500}\n",
      "{\"loss\": 2.555833255290985, \"learning_rate\": 1.5109725533345561e-05, \"epoch\": 0.7335411699981661, \"step\": 16000}\n",
      "{\"loss\": 2.414866141974926, \"learning_rate\": 1.4956904456262608e-05, \"epoch\": 0.7564643315606089, \"step\": 16500}\n",
      "{\"loss\": 2.507151679456234, \"learning_rate\": 1.4804083379179657e-05, \"epoch\": 0.7793874931230516, \"step\": 17000}\n",
      "{\"loss\": 2.422666306972504, \"learning_rate\": 1.4651262302096707e-05, \"epoch\": 0.8023106546854942, \"step\": 17500}\n",
      "{\"loss\": 2.4108485893309117, \"learning_rate\": 1.4498441225013756e-05, \"epoch\": 0.8252338162479369, \"step\": 18000}\n",
      "{\"loss\": 2.4431021788455545, \"learning_rate\": 1.4345620147930803e-05, \"epoch\": 0.8481569778103796, \"step\": 18500}\n",
      "{\"loss\": 2.452733239173889, \"learning_rate\": 1.4192799070847853e-05, \"epoch\": 0.8710801393728222, \"step\": 19000}\n",
      "{\"loss\": 2.414745060905814, \"learning_rate\": 1.4039977993764902e-05, \"epoch\": 0.894003300935265, \"step\": 19500}\n",
      "{\"loss\": 2.3531309423595665, \"learning_rate\": 1.388715691668195e-05, \"epoch\": 0.9169264624977077, \"step\": 20000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-20000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-20000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-20000\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.4539842493236064, \"learning_rate\": 1.3734335839598997e-05, \"epoch\": 0.9398496240601504, \"step\": 20500}\n",
      "{\"loss\": 2.405084967076778, \"learning_rate\": 1.3581514762516047e-05, \"epoch\": 0.962772785622593, \"step\": 21000}\n",
      "{\"loss\": 2.438625141851604, \"learning_rate\": 1.3428693685433096e-05, \"epoch\": 0.9856959471850357, \"step\": 21500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20e0fe36e67423989b198a597309717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=21812, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.3101954451948403, \"learning_rate\": 1.3275872608350145e-05, \"epoch\": 1.0086191087474785, \"step\": 22000}\n",
      "{\"loss\": 2.265027540564537, \"learning_rate\": 1.3123051531267192e-05, \"epoch\": 1.031542270309921, \"step\": 22500}\n",
      "{\"loss\": 2.2659556266069414, \"learning_rate\": 1.2970230454184242e-05, \"epoch\": 1.0544654318723639, \"step\": 23000}\n",
      "{\"loss\": 2.2776337320506572, \"learning_rate\": 1.281740937710129e-05, \"epoch\": 1.0773885934348066, \"step\": 23500}\n",
      "{\"loss\": 2.329091015994549, \"learning_rate\": 1.266458830001834e-05, \"epoch\": 1.1003117549972492, \"step\": 24000}\n",
      "{\"loss\": 2.2086111823022367, \"learning_rate\": 1.251176722293539e-05, \"epoch\": 1.123234916559692, \"step\": 24500}\n",
      "{\"loss\": 2.343381143331528, \"learning_rate\": 1.2358946145852437e-05, \"epoch\": 1.1461580781221345, \"step\": 25000}\n",
      "{\"loss\": 2.265583106189966, \"learning_rate\": 1.2206125068769485e-05, \"epoch\": 1.1690812396845773, \"step\": 25500}\n",
      "{\"loss\": 2.245767152041197, \"learning_rate\": 1.2053303991686534e-05, \"epoch\": 1.19200440124702, \"step\": 26000}\n",
      "{\"loss\": 2.251897181529552, \"learning_rate\": 1.1900482914603584e-05, \"epoch\": 1.2149275628094627, \"step\": 26500}\n",
      "{\"loss\": 2.2923901134431364, \"learning_rate\": 1.1747661837520633e-05, \"epoch\": 1.2378507243719055, \"step\": 27000}\n",
      "{\"loss\": 2.2659261505305768, \"learning_rate\": 1.159484076043768e-05, \"epoch\": 1.260773885934348, \"step\": 27500}\n",
      "{\"loss\": 2.245834646046162, \"learning_rate\": 1.1442019683354728e-05, \"epoch\": 1.2836970474967908, \"step\": 28000}\n",
      "{\"loss\": 2.2709990436434744, \"learning_rate\": 1.1289198606271779e-05, \"epoch\": 1.3066202090592334, \"step\": 28500}\n",
      "{\"loss\": 2.2459869550168516, \"learning_rate\": 1.1136377529188827e-05, \"epoch\": 1.3295433706216762, \"step\": 29000}\n",
      "{\"loss\": 2.2397707460075615, \"learning_rate\": 1.0983556452105874e-05, \"epoch\": 1.3524665321841187, \"step\": 29500}\n",
      "{\"loss\": 2.291079812049866, \"learning_rate\": 1.0830735375022925e-05, \"epoch\": 1.3753896937465615, \"step\": 30000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-30000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-30000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-30000\\pytorch_model.bin\n",
      "INFO:transformers.trainer:Deleting older checkpoint [myBERT_large\\checkpoint-10000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.333539779677987, \"learning_rate\": 1.0677914297939973e-05, \"epoch\": 1.3983128553090043, \"step\": 30500}\n",
      "{\"loss\": 2.2039798449352386, \"learning_rate\": 1.0525093220857022e-05, \"epoch\": 1.4212360168714469, \"step\": 31000}\n",
      "{\"loss\": 2.2049983873963357, \"learning_rate\": 1.0372272143774069e-05, \"epoch\": 1.4441591784338896, \"step\": 31500}\n",
      "{\"loss\": 2.2573918953416867, \"learning_rate\": 1.0219451066691119e-05, \"epoch\": 1.4670823399963324, \"step\": 32000}\n",
      "{\"loss\": 2.2568867580145597, \"learning_rate\": 1.0066629989608168e-05, \"epoch\": 1.490005501558775, \"step\": 32500}\n",
      "{\"loss\": 2.2566232802569868, \"learning_rate\": 9.913808912525216e-06, \"epoch\": 1.5129286631212175, \"step\": 33000}\n",
      "{\"loss\": 2.2700930643081665, \"learning_rate\": 9.760987835442265e-06, \"epoch\": 1.5358518246836603, \"step\": 33500}\n",
      "{\"loss\": 2.236307323265821, \"learning_rate\": 9.608166758359314e-06, \"epoch\": 1.5587749862461031, \"step\": 34000}\n",
      "{\"loss\": 2.2640343472361564, \"learning_rate\": 9.455345681276362e-06, \"epoch\": 1.5816981478085457, \"step\": 34500}\n",
      "{\"loss\": 2.2756479981429876, \"learning_rate\": 9.30252460419341e-06, \"epoch\": 1.6046213093709885, \"step\": 35000}\n",
      "{\"loss\": 2.267153945669532, \"learning_rate\": 9.14970352711046e-06, \"epoch\": 1.6275444709334312, \"step\": 35500}\n",
      "{\"loss\": 2.1726719863712787, \"learning_rate\": 8.996882450027508e-06, \"epoch\": 1.6504676324958738, \"step\": 36000}\n",
      "{\"loss\": 2.2615421395106243, \"learning_rate\": 8.844061372944557e-06, \"epoch\": 1.6733907940583164, \"step\": 36500}\n",
      "{\"loss\": 2.244652558505535, \"learning_rate\": 8.691240295861605e-06, \"epoch\": 1.6963139556207594, \"step\": 37000}\n",
      "{\"loss\": 2.2347954516112805, \"learning_rate\": 8.538419218778656e-06, \"epoch\": 1.719237117183202, \"step\": 37500}\n",
      "{\"loss\": 2.2448704336881637, \"learning_rate\": 8.385598141695703e-06, \"epoch\": 1.7421602787456445, \"step\": 38000}\n",
      "{\"loss\": 2.1605265151113273, \"learning_rate\": 8.232777064612753e-06, \"epoch\": 1.7650834403080873, \"step\": 38500}\n",
      "{\"loss\": 2.2794624360091986, \"learning_rate\": 8.0799559875298e-06, \"epoch\": 1.78800660187053, \"step\": 39000}\n",
      "{\"loss\": 2.1518266795277596, \"learning_rate\": 7.92713491044685e-06, \"epoch\": 1.8109297634329726, \"step\": 39500}\n",
      "{\"loss\": 2.2795813713669775, \"learning_rate\": 7.774313833363897e-06, \"epoch\": 1.8338529249954154, \"step\": 40000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-40000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-40000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-40000\\pytorch_model.bin\n",
      "INFO:transformers.trainer:Deleting older checkpoint [myBERT_large\\checkpoint-20000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.3060937882810832, \"learning_rate\": 7.6214927562809475e-06, \"epoch\": 1.8567760865578582, \"step\": 40500}\n",
      "{\"loss\": 2.188187908321619, \"learning_rate\": 7.468671679197995e-06, \"epoch\": 1.8796992481203008, \"step\": 41000}\n",
      "{\"loss\": 2.2936088038384916, \"learning_rate\": 7.315850602115045e-06, \"epoch\": 1.9026224096827433, \"step\": 41500}\n",
      "{\"loss\": 2.2142816158384084, \"learning_rate\": 7.1630295250320925e-06, \"epoch\": 1.925545571245186, \"step\": 42000}\n",
      "{\"loss\": 2.204173205435276, \"learning_rate\": 7.010208447949142e-06, \"epoch\": 1.948468732807629, \"step\": 42500}\n",
      "{\"loss\": 2.1612360605448484, \"learning_rate\": 6.857387370866191e-06, \"epoch\": 1.9713918943700715, \"step\": 43000}\n",
      "{\"loss\": 2.2009097110033036, \"learning_rate\": 6.704566293783239e-06, \"epoch\": 1.9943150559325142, \"step\": 43500}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7861f4f99444c11803e8dbf5388c904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=21812, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.1078995309770105, \"learning_rate\": 6.551745216700288e-06, \"epoch\": 2.017238217494957, \"step\": 44000}\n",
      "{\"loss\": 2.1744367275834082, \"learning_rate\": 6.3989241396173365e-06, \"epoch\": 2.0401613790573996, \"step\": 44500}\n",
      "{\"loss\": 2.1157472562491892, \"learning_rate\": 6.246103062534385e-06, \"epoch\": 2.063084540619842, \"step\": 45000}\n",
      "{\"loss\": 2.1088514780700205, \"learning_rate\": 6.093281985451434e-06, \"epoch\": 2.086007702182285, \"step\": 45500}\n",
      "{\"loss\": 2.1483673489689825, \"learning_rate\": 5.940460908368482e-06, \"epoch\": 2.1089308637447277, \"step\": 46000}\n",
      "{\"loss\": 2.1313816441185773, \"learning_rate\": 5.787639831285531e-06, \"epoch\": 2.1318540253071703, \"step\": 46500}\n",
      "{\"loss\": 2.0758882296085357, \"learning_rate\": 5.63481875420258e-06, \"epoch\": 2.1547771868696133, \"step\": 47000}\n",
      "{\"loss\": 2.096497130870819, \"learning_rate\": 5.481997677119629e-06, \"epoch\": 2.177700348432056, \"step\": 47500}\n",
      "{\"loss\": 2.0283549708127975, \"learning_rate\": 5.329176600036678e-06, \"epoch\": 2.2006235099944984, \"step\": 48000}\n",
      "{\"loss\": 2.170327884018421, \"learning_rate\": 5.176355522953726e-06, \"epoch\": 2.223546671556941, \"step\": 48500}\n",
      "{\"loss\": 2.0663234624266624, \"learning_rate\": 5.023534445870775e-06, \"epoch\": 2.246469833119384, \"step\": 49000}\n",
      "{\"loss\": 2.06389231723547, \"learning_rate\": 4.870713368787824e-06, \"epoch\": 2.2693929946818265, \"step\": 49500}\n",
      "{\"loss\": 2.086746433040127, \"learning_rate\": 4.717892291704872e-06, \"epoch\": 2.292316156244269, \"step\": 50000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-50000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-50000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-50000\\pytorch_model.bin\n",
      "INFO:transformers.trainer:Deleting older checkpoint [myBERT_large\\checkpoint-30000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.1159435301516205, \"learning_rate\": 4.565071214621921e-06, \"epoch\": 2.3152393178067117, \"step\": 50500}\n",
      "{\"loss\": 2.095642631396651, \"learning_rate\": 4.4122501375389696e-06, \"epoch\": 2.3381624793691547, \"step\": 51000}\n",
      "{\"loss\": 2.13258510017395, \"learning_rate\": 4.259429060456018e-06, \"epoch\": 2.3610856409315972, \"step\": 51500}\n",
      "{\"loss\": 2.089134459018707, \"learning_rate\": 4.106607983373067e-06, \"epoch\": 2.38400880249404, \"step\": 52000}\n",
      "{\"loss\": 2.1241973213031886, \"learning_rate\": 3.9537869062901154e-06, \"epoch\": 2.406931964056483, \"step\": 52500}\n",
      "{\"loss\": 2.1201589764542876, \"learning_rate\": 3.8009658292071645e-06, \"epoch\": 2.4298551256189254, \"step\": 53000}\n",
      "{\"loss\": 2.0934932481348514, \"learning_rate\": 3.648144752124213e-06, \"epoch\": 2.452778287181368, \"step\": 53500}\n",
      "{\"loss\": 2.147756223157048, \"learning_rate\": 3.495323675041262e-06, \"epoch\": 2.475701448743811, \"step\": 54000}\n",
      "{\"loss\": 2.1293551406562328, \"learning_rate\": 3.342502597958311e-06, \"epoch\": 2.4986246103062535, \"step\": 54500}\n",
      "{\"loss\": 2.0943169203698635, \"learning_rate\": 3.1896815208753594e-06, \"epoch\": 2.521547771868696, \"step\": 55000}\n",
      "{\"loss\": 2.0998255264759065, \"learning_rate\": 3.036860443792408e-06, \"epoch\": 2.544470933431139, \"step\": 55500}\n",
      "{\"loss\": 2.062360143005848, \"learning_rate\": 2.8840393667094567e-06, \"epoch\": 2.5673940949935816, \"step\": 56000}\n",
      "{\"loss\": 2.0038401518166067, \"learning_rate\": 2.7312182896265053e-06, \"epoch\": 2.590317256556024, \"step\": 56500}\n",
      "{\"loss\": 2.030984899222851, \"learning_rate\": 2.578397212543554e-06, \"epoch\": 2.6132404181184667, \"step\": 57000}\n",
      "{\"loss\": 2.0628653721511365, \"learning_rate\": 2.425576135460603e-06, \"epoch\": 2.6361635796809098, \"step\": 57500}\n",
      "{\"loss\": 2.0716449189186097, \"learning_rate\": 2.2727550583776516e-06, \"epoch\": 2.6590867412433523, \"step\": 58000}\n",
      "{\"loss\": 2.07294765727222, \"learning_rate\": 2.1199339812947003e-06, \"epoch\": 2.682009902805795, \"step\": 58500}\n",
      "{\"loss\": 1.9880788303613663, \"learning_rate\": 1.967112904211749e-06, \"epoch\": 2.7049330643682374, \"step\": 59000}\n",
      "{\"loss\": 2.020923106118571, \"learning_rate\": 1.8142918271287975e-06, \"epoch\": 2.7278562259306804, \"step\": 59500}\n",
      "{\"loss\": 1.9932396007329225, \"learning_rate\": 1.6614707500458466e-06, \"epoch\": 2.750779387493123, \"step\": 60000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\\checkpoint-60000\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\checkpoint-60000\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\checkpoint-60000\\pytorch_model.bin\n",
      "INFO:transformers.trainer:Deleting older checkpoint [myBERT_large\\checkpoint-40000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 2.0714624010026457, \"learning_rate\": 1.5086496729628952e-06, \"epoch\": 2.7737025490555656, \"step\": 60500}\n",
      "{\"loss\": 2.0736990784406664, \"learning_rate\": 1.3558285958799439e-06, \"epoch\": 2.7966257106180086, \"step\": 61000}\n",
      "{\"loss\": 1.9879294981360436, \"learning_rate\": 1.2030075187969925e-06, \"epoch\": 2.819548872180451, \"step\": 61500}\n",
      "{\"loss\": 2.089942114531994, \"learning_rate\": 1.0501864417140413e-06, \"epoch\": 2.8424720337428937, \"step\": 62000}\n",
      "{\"loss\": 2.0671531477170064, \"learning_rate\": 8.9736536463109e-07, \"epoch\": 2.8653951953053367, \"step\": 62500}\n",
      "{\"loss\": 2.0627119898572563, \"learning_rate\": 7.445442875481387e-07, \"epoch\": 2.8883183568677793, \"step\": 63000}\n",
      "{\"loss\": 2.153741326868534, \"learning_rate\": 5.917232104651874e-07, \"epoch\": 2.911241518430222, \"step\": 63500}\n",
      "{\"loss\": 2.0252604929804803, \"learning_rate\": 4.389021333822361e-07, \"epoch\": 2.934164679992665, \"step\": 64000}\n",
      "{\"loss\": 2.1637557763159276, \"learning_rate\": 2.860810562992848e-07, \"epoch\": 2.9570878415551074, \"step\": 64500}\n",
      "{\"loss\": 2.0033474916219713, \"learning_rate\": 1.3325997921633353e-07, \"epoch\": 2.98001100311755, \"step\": 65000}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.trainer:\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "INFO:transformers.trainer:Saving model checkpoint to ./myBERT_large\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./myBERT_large\\config.json\n",
      "INFO:transformers.modeling_utils:Model weights saved in ./myBERT_large\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./myBERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_gpu_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-05,\n",
    "    no_cuda=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=fine_tuning_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./myBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tag + Sentence Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query2maskedQueries(query, tokenizer, mask_token='[MASK]'):\n",
    "    # tokenize and POS tagging\n",
    "    words = tokenizer.tokenize(query)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    sentence_len = len(words)\n",
    "    # mask each word\n",
    "    for i in range(sentence_len):\n",
    "        words[i] = mask_token\n",
    "        if i <= sentence_len / 3:\n",
    "            yield [' '.join(words), tags[i][0], tags[i][1] + ' head']\n",
    "        elif i > sentence_len / 3 * 2:\n",
    "            yield [' '.join(words), tags[i][0], tags[i][1] + ' tail']\n",
    "        else:\n",
    "            yield [' '.join(words), tags[i][0], tags[i][1] + ' middle']\n",
    "        words[i] = tags[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../dataset/geo/testset.txt', 'r', encoding='utf-8') as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPU\n",
    "\n",
    "def predict(queries, model, tokenizer, topk=10):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fill_mask = pipeline(\n",
    "            \"fill-mask\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            topk=topk,\n",
    "            device=0\n",
    "        )\n",
    "        # predicting\n",
    "        predict_tokens, ground_truth, tagsNpositions = [], [], []\n",
    "        \n",
    "        for query in tqdm(queries):\n",
    "            _sentences = []\n",
    "            for sentence, label, tagsNposition in query2maskedQueries(query, tokenizer):\n",
    "                if label[0].isalpha():\n",
    "                    _sentences.append(sentence)\n",
    "                    ground_truth.append(label)\n",
    "                    tagsNpositions.append(tagsNposition)\n",
    "            for query_res in fill_mask(_sentences):\n",
    "                predict_tokens += tokenizer.convert_ids_to_tokens([pred['token'] for pred in query_res])\n",
    "\n",
    "    return predict_tokens, ground_truth, tagsNpositions\n",
    "\n",
    "def predict_low_speed(queries, model, tokenizer, mask='[MASK]', topk=10):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fill_mask = pipeline(\n",
    "            \"fill-mask\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            topk=topk,\n",
    "            device=0\n",
    "        )\n",
    "        # predicting\n",
    "        predict_tokens, ground_truth, tagsNpositions = [], [], []\n",
    "        \n",
    "        _sentences = []\n",
    "        for query in queries:\n",
    "            for sentence, label, tagsNposition in query2maskedQueries(query, tokenizer, mask):\n",
    "                if label[0].isalpha() or label[0] == '▁' or label[0] == 'Ġ':\n",
    "                    _sentences.append(sentence)\n",
    "                    ground_truth.append(label)\n",
    "                    tagsNpositions.append(tagsNposition)\n",
    "        \n",
    "        stride = 10\n",
    "        for i in tqdm(range(0, len(_sentences), stride)):\n",
    "            end = i + stride\n",
    "            if end > len(_sentences):\n",
    "                end = len(_sentences)\n",
    "            for query_res in fill_mask(_sentences[i:end]):\n",
    "                if isinstance(query_res, list):\n",
    "                    predict_tokens += tokenizer.convert_ids_to_tokens([pred['token'] for pred in query_res])\n",
    "                else:\n",
    "                    predict_tokens += tokenizer.convert_ids_to_tokens([query_res['token']])\n",
    "\n",
    "    return predict_tokens, ground_truth, tagsNpositions\n",
    "\n",
    "def embedding_cache(embedding_model, embedding_tokenizer, words):\n",
    "    cache = {}\n",
    "    embedding_model = embedding_model.to('cuda')\n",
    "    stride = 100\n",
    "    for i in tqdm(range(0, len(words), stride)):\n",
    "        end = i + stride\n",
    "        if end > len(words):\n",
    "            end = len(words)\n",
    "        ids = torch.tensor([embedding_tokenizer.convert_tokens_to_ids(words[i:end])]).to('cuda')\n",
    "        embeds = embedding_model(ids)[0][0].cpu().detach().numpy()\n",
    "        for j, word in enumerate(words[i:end]):\n",
    "            cache[word] = embeds[j]\n",
    "    return cache\n",
    "    \n",
    "def evaluate(predict_tokens, ground_truth, tagsNpositions, embedding_model, embedding_tokenizer):\n",
    "    results = {}\n",
    "    type_count = {}\n",
    "    # embedding\n",
    "    words = list(set(predict_tokens + ground_truth))\n",
    "    cache = embedding_cache(embedding_model, embedding_tokenizer, words)\n",
    "    \n",
    "    for i in tqdm(range(len(ground_truth))):\n",
    "        cos_sim = []\n",
    "        ground_truth_embedding = cache[ground_truth[i]]\n",
    "        tagsNposition = tagsNpositions[i]\n",
    "        if tagsNposition not in results:\n",
    "            results[tagsNposition] = 0\n",
    "            type_count[tagsNposition] = 0\n",
    "        for predict in predict_tokens[i*10:(i+1)*10]:\n",
    "            predict_embedding = cache[predict]\n",
    "            cos_sim.append(np.dot(ground_truth_embedding, predict_embedding) / np.linalg.norm(ground_truth_embedding) / np.linalg.norm(predict_embedding))\n",
    "        results[tagsNposition] += max(cos_sim)\n",
    "        type_count[tagsNposition] += 1\n",
    "\n",
    "    for key in results.keys():\n",
    "        results[key] /= type_count[key]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedding_model = pretrained_model\n",
    "embedding_tokenizer = pretrained_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "embedding_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "embedding_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on this model!\n",
    "pretrained_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27075/27075 [17:44<00:00, 25.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 140/140 [00:03<00:00, 39.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 270745/270745 [00:24<00:00, 10925.69it/s]\n"
     ]
    }
   ],
   "source": [
    "print('testing pretrained model')\n",
    "predict_tokens, ground_truth, tagsNpositions = predict_low_speed(queries, pretrained_model, pretrained_tokenizer)\n",
    "result = evaluate(predict_tokens, ground_truth, tagsNpositions, embedding_model, embedding_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file ./myBERT\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file ./myBERT\\pytorch_model.bin\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "fine_tuning_model = BertForMaskedLM.from_pretrained('./myBERT')\n",
    "fine_tuning_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "embedding_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "embedding_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing fine-tuned model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27075/27075 [32:07<00:00, 14.05it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 203/203 [00:05<00:00, 38.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 270745/270745 [00:24<00:00, 10972.14it/s]\n"
     ]
    }
   ],
   "source": [
    "print('testing fine-tuned model')\n",
    "\n",
    "predict_tokens, ground_truth, tagsNpositions = predict_low_speed(queries, fine_tuning_model, fine_tuning_tokenizer)\n",
    "result = evaluate(predict_tokens, ground_truth, tagsNpositions, embedding_model, embedding_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = {}\n",
    "POS_counter = {}\n",
    "for tagsNposition in tagsNpositions:\n",
    "    if tagsNposition not in counter:\n",
    "        counter[tagsNposition] = 0\n",
    "    POS = tagsNposition.split(' ')[0]\n",
    "    if POS not in POS_counter:\n",
    "        POS_counter[POS] = 0\n",
    "    counter[tagsNposition] += 1\n",
    "    POS_counter[POS] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN head 22859 0.8578997150384888\n",
      "NN middle 20637 0.8571619530033625\n",
      "NN tail 20107 0.8576473644372985\n",
      "IN head 15035 0.8350105194542786\n",
      "DT head 13955 0.8118574367035934\n",
      "IN middle 13521 0.8338719840083599\n",
      "JJ head 13398 0.8542414535994244\n",
      "IN tail 11677 0.8352068005241514\n",
      "JJ middle 11555 0.854528875133786\n",
      "JJ tail 10762 0.855617246914034\n",
      "DT middle 10141 0.8118928954065486\n",
      "DT tail 8008 0.8117925959316405\n",
      "NNS head 6931 0.860105493680651\n",
      "NNS tail 6712 0.8604160737561526\n",
      "NNS middle 6476 0.8600790793924069\n",
      "VBD head 5367 0.8544084299414745\n",
      "CC middle 5146 0.8021190590516538\n",
      "CC tail 4329 0.7994905543756915\n",
      "RB head 4265 0.8557051723475753\n",
      "VBD middle 4132 0.8560149538522769\n",
      "RB middle 3292 0.8535131675577106\n",
      "VBN head 3181 0.8649062462776421\n",
      "CC head 2931 0.8034162332441407\n",
      "VBN middle 2790 0.8654335799823952\n",
      "RB tail 2704 0.8534517345740598\n",
      "VBD tail 2684 0.8546848179864102\n",
      "VBN tail 2192 0.8661425693137367\n",
      "TO middle 2167 0.81955615754605\n",
      "VBZ head 1915 0.8536311236436311\n",
      "VB middle 1838 0.8517110127230074\n",
      "TO tail 1837 0.8193658024121317\n",
      "TO head 1769 0.8201439768463423\n",
      "VB tail 1734 0.8521762149969186\n",
      "VB head 1396 0.8482988807652946\n",
      "VBP head 1394 0.8319775466853954\n",
      "VBG middle 1306 0.8657589857830206\n",
      "VBZ middle 1137 0.8530620158295309\n",
      "VBG head 1092 0.8669330988065663\n",
      "PRP head 1078 0.8498870805044121\n",
      "VBP middle 1066 0.8307207081935195\n",
      "VBG tail 1042 0.8665235508769579\n",
      "PRP middle 885 0.8481285931700367\n",
      "PRP$ middle 833 0.8310191453862734\n",
      "PRP$ tail 818 0.8310372935822075\n",
      "PRP$ head 776 0.8302568781007197\n",
      "WDT middle 769 0.8129237769329036\n",
      "VBP tail 664 0.8313100214105055\n",
      "VBZ tail 644 0.8544746461029379\n",
      "CD head 640 0.8504014638718218\n",
      "PRP tail 596 0.8476562122970619\n",
      "CD middle 511 0.8467111859886147\n",
      "JJS head 455 0.8447315539632525\n",
      "WDT tail 447 0.8120491594009485\n",
      "WDT head 428 0.8095762793705842\n",
      "CD tail 410 0.847467796322776\n",
      "JJR head 362 0.8530796606567025\n",
      "JJR middle 351 0.8566882887117544\n",
      "JJR tail 343 0.8522787721442064\n",
      "JJS middle 334 0.8480969217723001\n",
      "MD head 324 0.8615023252772697\n",
      "MD middle 297 0.8606741516678421\n",
      "JJS tail 294 0.8485159606349711\n",
      "WP middle 288 0.6427939302391477\n",
      "EX head 240 0.8530490398406982\n",
      "MD tail 240 0.8623911996682485\n",
      "WRB head 239 0.8676661958754313\n",
      "WRB middle 200 0.8672199335694313\n",
      "WP head 193 0.6613683734533082\n",
      "WP tail 160 0.6712638061493635\n",
      "RP middle 155 0.8569275036934884\n",
      "RBR middle 153 0.8483510500465343\n",
      "RBR head 151 0.8524393652448591\n",
      "FW middle 148 0.8570628613233566\n",
      "RBS head 146 0.8351029251536278\n",
      "RP tail 132 0.8501158519224687\n",
      "RBR tail 132 0.8484748041991031\n",
      "FW head 127 0.8513458136498459\n",
      "RP head 117 0.8519073205116467\n",
      "FW tail 109 0.8499773199405145\n",
      "WRB tail 98 0.865594992224051\n",
      "NNP head 91 0.8569431331131484\n",
      "EX middle 90 0.851301501194636\n",
      "RBS tail 90 0.8348795791467031\n",
      "RBS middle 83 0.8360606740756207\n",
      "NNP middle 82 0.8566303035108055\n",
      "NNP tail 74 0.8541519327743633\n",
      "PDT head 72 0.8336960060728921\n",
      "SYM middle 65 0.8430857154039236\n",
      "SYM head 61 0.8368453031680623\n",
      "WP$ middle 46 0.8219156330046447\n",
      "SYM tail 34 0.8466943221933702\n",
      "PDT middle 32 0.8292449433356524\n",
      "EX tail 31 0.8537133182248762\n",
      "WP$ head 21 0.8263842264811198\n",
      "PDT tail 19 0.8325146875883404\n",
      "WP$ tail 18 0.8167429400814904\n",
      "$ head 13 0.854845601778764\n",
      "$ tail 11 0.8625907897949219\n",
      "NNPS middle 10 0.8811231672763824\n",
      "$ middle 10 0.8361913025379181\n",
      "NNPS head 9 0.8746969236267937\n",
      "UH middle 6 0.8705801069736481\n",
      "UH tail 4 0.8693358451128006\n",
      "NNPS tail 3 0.8819884657859802\n",
      "UH head 2 0.8644079267978668\n",
      "POS head 1 0.8854566216468811\n"
     ]
    }
   ],
   "source": [
    "total_score = 0\n",
    "for k in sorted(counter, key=lambda k: -counter[k]):\n",
    "    print(k, counter[k], result[k])\n",
    "    total_score += counter[k] * result[k]\n",
    "total_score /= sum(counter.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8435845623789262"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_result = {}\n",
    "POS_count = {}\n",
    "position_result = {}\n",
    "position_count = {}\n",
    "for key in result.keys():\n",
    "    POS, position = key.split(' ')\n",
    "    if POS not in POS_result:\n",
    "        POS_result[POS] = 0\n",
    "        POS_count[POS] = 0\n",
    "    if position not in position_result:\n",
    "        position_result[position] = 0\n",
    "        position_count[position] = 0\n",
    "    POS_result[POS] += result[key]\n",
    "    POS_count[POS] += 1\n",
    "    position_result[position] += result[key]\n",
    "    position_count[position] += 1\n",
    "\n",
    "for key in POS_result.keys():\n",
    "    POS_result[key] /= POS_count[key]\n",
    "for key in position_result.keys():\n",
    "    position_result[key] /= position_count[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'IN': 0.8164507228091029,\n",
       " 'DT': 0.7590527545858299,\n",
       " 'JJ': 0.854006506637432,\n",
       " 'NN': 0.8548732210070132,\n",
       " 'VBZ': 0.8516416271741537,\n",
       " 'RB': 0.8278739336735561,\n",
       " 'VBN': 0.8611208665360882,\n",
       " 'NNS': 0.8564696598191972,\n",
       " 'CC': 0.8151586679297947,\n",
       " 'VBP': 0.7262765500757057,\n",
       " 'TO': 0.8601523128120591,\n",
       " 'VB': 0.8543623682908379,\n",
       " 'PRP': 0.8486366122051187,\n",
       " 'VBG': 0.867899539820035,\n",
       " 'JJS': 0.8450265457934139,\n",
       " 'RBS': 0.8071210239999979,\n",
       " 'VBD': 0.8386600828467209,\n",
       " 'PRP$': 0.8506894727507722,\n",
       " 'SYM': 0.8522878110293828,\n",
       " 'JJR': 0.8531515305006355,\n",
       " 'FW': 0.8507318463701611,\n",
       " 'WP': 0.8725918824923683,\n",
       " 'MD': 0.8493478157863806,\n",
       " 'WRB': 0.8354810706960251,\n",
       " 'RP': 0.8587108361629259,\n",
       " 'WDT': 0.8391820580773115,\n",
       " 'CD': 0.8263817516744624,\n",
       " 'WP$': 0.7968932658893255,\n",
       " 'PDT': 0.8522027058356222,\n",
       " 'NNP': 0.8536616961702088,\n",
       " 'RBR': 0.8549123446479031,\n",
       " 'EX': 0.7089451461718856,\n",
       " 'NNPS': 0.8293821703504634,\n",
       " '$': 0.8557611788764143,\n",
       " 'UH': 0.8578662392165927,\n",
       " 'POS': 0.9132225513458252}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'head': 0.8374526887491365,\n",
       " 'middle': 0.836231625667916,\n",
       " 'tail': 0.8350202946898442}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file ./myBERT\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file ./myBERT\\pytorch_model.bin\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('./myBERT')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file ./myBERT_large\\config.json\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file ./myBERT_large\\pytorch_model.bin\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('./myBERT_large')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\6dfaed860471b03ab5b9acb6153bea82b6632fb9bbe514d3fff050fe1319ee6d.788fed32bb8481a9b15ce726d41c53d5d5066b04c667e34ce3a7a3826d1573d8\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-large-uncased-pytorch_model.bin from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\73e65a4648c1a5eab31ecea94e04a92a7168cd7089d588b68e5bc057aff40421.4d5343a4b979c4beeaadef17a0453d1bb183dd9b084f58b84c7cc781df343ae6\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt from cache at C:\\Users\\Administrator/.cache\\torch\\transformers\\9b3c03a36e83b13d5ba95ac965c9f9074a99e14340c523ab405703179e79fc46.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "pretrained_model = BertModel.from_pretrained('bert-large-uncased')\n",
    "pretrained_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dataset', 'r', encoding='utf-8') as f:\n",
    "    baseset = [line.strip() for line in f.readlines()]\n",
    "with open('data/ann_set', 'r', encoding='utf-8') as f:\n",
    "    queryset = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask = pipeline('feature-extraction', model=pretrained_model, tokenizer=pretrained_tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5462/5462 [08:09<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 54617 dimension = 1024\n"
     ]
    }
   ],
   "source": [
    "base_embeddings = []\n",
    "stride = 10\n",
    "for i in tqdm(range(0, len(baseset), stride)):\n",
    "    end = i + stride\n",
    "    if end > len(baseset):\n",
    "        end = len(baseset)\n",
    "    for embed in fill_mask(baseset[i:end]):\n",
    "        base_embeddings.append(embed[0])\n",
    "print('n =', len(base_embeddings), 'dimension =', len(base_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5462/5462 [08:43<00:00, 10.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 54618 dimension = 1024\n"
     ]
    }
   ],
   "source": [
    "query_embeddings = []\n",
    "stride = 10\n",
    "for i in tqdm(range(0, len(queryset), stride)):\n",
    "    end = i + stride\n",
    "    if end > len(queryset):\n",
    "        end = len(queryset)\n",
    "    for embed in fill_mask(queryset[i:end]):\n",
    "        query_embeddings.append(embed[0])\n",
    "print('n =', len(query_embeddings), 'dimension =', len(query_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors=10, algorithm='ball_tree').fit(np.array(base_embeddings))\n",
    "distances, indices = nbrs.kneighbors(np.array(query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5262367717602255\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, index in enumerate(indices):\n",
    "    if i in index:\n",
    "        count += 1\n",
    "print(count/len(indices))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
